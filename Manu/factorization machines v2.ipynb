{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.impute import SimpleImputer\nimport xgboost as xgb\nfrom sklearn.preprocessing import OneHotEncoder\nfrom category_encoders import CountEncoder\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import Audio\nimport matplotlib.pyplot as plt\nfrom fastFM import als\nfrom category_encoders import CatBoostEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Audio\n# Cargo los archivos\nsound_file = '/kaggle/input/pingsound/PING - Sound effect.mp3'\ndef ping():\n    display(Audio(sound_file, autoplay=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es_para_submit = True\nagregar_estimacion_preciom2 = True\nif es_para_submit: \n    TAMANIO_TRAIN = 240000\n    TAMANIO_TEST = 60000\nelse:\n    TAMANIO_TRAIN = 190000\n    TAMANIO_TEST = 50000","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/tp2datostrain/test.csv\")\ndisplay(test.shape)\ntrain = pd.read_csv(\"../input/tp2datostrain/train.csv\")\ndisplay(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ciudades = list(train[\"ciudad\"].unique())\ntipos_de_propiedad = list(train[\"tipodepropiedad\"].unique())\nprovincia = list(train[\"provincia\"].unique())\ncategorias  = [provincia, tipos_de_propiedad, ciudades]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [#cOLUMNAS\n            'idzona',\n            'lat', 'lng',\n            \"metrostotales\", \"metroscubiertos\", \n            'garages', 'banos',\n            'gimnasio','centroscomercialescercanos','usosmultiples','piscina','escuelascercanas',\n            'antiguedad',\n            'habitaciones',\n            'provincia', 'tipodepropiedad', 'ciudad',\n           ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformador_categorias = make_pipeline(SimpleImputer(strategy='most_frequent'),\n                                         OneHotEncoder(handle_unknown='ignore', categories = categorias))\ntransformador_numeros = make_pipeline(SimpleImputer())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def obtenerFeaturesCategoricos(features):\n    l = []\n    for feature in features:\n        if not np.issubdtype(df[feature].dtype, np.number):\n            l.append(feature)\n    return l\n\nnum_features = list(df[features].select_dtypes(include=[np.number]).columns)\n\ncat_features = list(obtenerFeaturesCategoricos(features))\nprint (\"Features numericos:\", num_features, '\\nFeatures categoricos: ', cat_features)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', transformador_numeros, num_features),\n        ('cat', transformador_categorias, cat_features)\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[features].copy()\ny = train[\"precio\"].copy()\ntrain_x, test_x, train_y, test_y = train_test_split(X, y,train_size=0.8, test_size=0.2)\nfrom fastFM import als\nfm = als.FMRegression(n_iter=1000, init_stdev=0.1, rank=2, l2_reg_w=0.1, l2_reg_V=0.5)\nx_procesada = preprocessor.fit_transform(train_x)\nx_test_procesada = preprocessor.fit_transform(test_x)\n\nfm.fit(x_procesada, train_y)\ny_pred = fm.predict(x_test_procesada)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ping()\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_pred, test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[features].copy()\ny = train[\"precio\"].copy()\ntrain_x, test_x, train_y, test_y = train_test_split(X, y,train_size=0.8, test_size=0.2)\nfrom fastFM import sgd\nfm2 = sgd.FMRegression(n_iter=1000, init_stdev=0.1, rank=2, step_size=0.00000000001)\nx_procesada = preprocessor.fit_transform(train_x)\nx_test_procesada = preprocessor.fit_transform(test_x)\n\nfm2.fit(x_procesada, train_y)\ny_pred2 = fm2.predict(x_test_procesada)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ping()\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_pred2, test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PAra predecir los precios con los features normales no sirve de mucho FM\n\nPruebo un poco con el preprocesamiento y features mas copados"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfCheto = pd.concat([train,test], sort=False)\ndfCheto['precio'] = SimpleImputer(strategy='median').fit_transform(dfCheto[['precio']])\ndf = dfCheto.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.provincia.fillna('provinciaVacia', inplace = True)\naux2 = df[['ciudad', 'provincia']].copy()\naux2.dropna(inplace = True)\naux2 = aux2.groupby(['provincia']).agg(lambda x:x.provincia.value_counts().index[0]).reset_index().set_index('ciudad')\nciuds = aux2.T.to_dict('records').copy()\nciuds = ciuds[0]\ndf.ciudad.fillna(df.provincia.map(ciuds), inplace = True)\ndf.ciudad.fillna('ciudadVacia', inplace=True)\n\ndf.tipodepropiedad.fillna('tipoVacio', inplace=True)\n\naux2 = df[['idzona', 'ciudad']].copy()\naux2.dropna(inplace = True)\naux2 = aux2.groupby('ciudad').agg({'idzona':'median'})\nids = aux2.T.to_dict('records').copy()\nids = ids[0]\ndf.idzona.fillna(df.ciudad.map(ids), inplace = True)\ndf['idzona'] = SimpleImputer(strategy='most_frequent').fit_transform(df[['idzona']])\n\n#Adicionales\naux = df.copy()\naux.descripcion.fillna('',inplace=True)\naux['jardin'] = (aux.descripcion.str.contains('jardin') | aux.descripcion.str.contains('jardín')).astype(int)\naux['vigilancia'] = (aux.descripcion.str.contains('vigilancia') | aux.descripcion.str.contains('seguridad')).astype(int) \naux['buenaUbicacion'] = (aux.descripcion.str.contains('ubicacion') | aux.descripcion.str.contains('ubicación')).astype(int)\naux['lavadero'] = (aux.descripcion.str.contains('lavadero') | aux.descripcion.str.contains('lavado')).astype(int)\naux['jacuzzi'] = (aux.descripcion.str.contains('jacuzzi')).astype(int)\ndef buscar_barrioCerrado(string):\n    return int(('barrio cerrado' in string) or ('urbanizacion cerrada' in string) or ('urbanización cerrada' in string)\n    or ('barrio residencial' in string) or ('country' in string) or ('barrio privado' in string) or ('colonia privada' in string))\naux['barrio_cerrado'] = aux.descripcion.map(lambda x:buscar_barrioCerrado(str(x)))\n\nvalor_adicionales = {'gimnasio':4,'usosmultiples':2,'piscina':5,'escuelascercanas':3,'centroscomercialescercanos':3,\n                     'buenaUbicacion':2,'vigilancia': 5,'jardin':4,'jacuzzi':3,'lavadero':0.5,'barrio_cerrado':10}\n            \nfor columna in aux.columns:\n    if columna in valor_adicionales:\n        aux[columna] = aux[columna].map(lambda x: x*valor_adicionales[columna])\n\naux['adicionales'] = aux[valor_adicionales.keys()].sum(axis=1)\ndf['adicionales'] = aux['adicionales']\n\n\naux = df[['lat', 'ciudad']].copy()\naux.dropna(inplace = True)\naux = aux.groupby('ciudad').agg({'lat':'mean'})\nlats = aux.T.to_dict('records').copy()\nlats = lats[0]\ndf.lat.fillna(df.ciudad.map(lats), inplace = True)\ndf['lat'] = SimpleImputer(strategy='most_frequent').fit_transform(df[['lat']])\n\naux = df[['lng', 'ciudad']].copy()\naux.dropna(inplace = True)\naux = aux.groupby('ciudad').agg({'lng':'mean'})\nlngs = aux.T.to_dict('records').copy()\nlngs = lngs[0]\ndf.lng.fillna(df.ciudad.map(lngs), inplace = True)\ndf['lng'] = SimpleImputer(strategy='most_frequent').fit_transform(df[['lng']])\n\naux = df[['tipodepropiedad', 'habitaciones']].copy()\naux.dropna(inplace = True)\naux = aux.groupby('tipodepropiedad').agg({'habitaciones':'median'})\nhabts = aux.T.to_dict('records').copy()\nhabts = habts[0]\ndf.habitaciones.fillna(df.tipodepropiedad.map(habts), inplace = True)\ndf['habitaciones'] = SimpleImputer(strategy='most_frequent').fit_transform(df[['habitaciones']])\n\naux = df[['tipodepropiedad', 'garages']].copy()\naux.dropna(inplace = True)\naux = aux.groupby('tipodepropiedad').agg({'garages':'median'})\nhabts = aux.T.to_dict('records').copy()\nhabts = habts[0]\ndf.garages.fillna(df.tipodepropiedad.map(habts), inplace = True)\ndf.garages.fillna(0, inplace=True)\n\ndf.drop(columns=['direccion'], inplace=True)\n\n\naux = df[['antiguedad', 'ciudad']].copy()\naux.dropna(inplace = True)\naux = aux.groupby('ciudad').agg({'antiguedad':'mean'})\nlngs = aux.T.to_dict('records').copy()\nlngs = lngs[0]\ndf.antiguedad.fillna(df.ciudad.map(lngs), inplace = True)\ndf['antiguedad'] = SimpleImputer(strategy='most_frequent').fit_transform(df[['antiguedad']])\n\naux = df[['banos', 'tipodepropiedad']].copy()\naux.dropna(inplace = True)\naux = aux.groupby('tipodepropiedad').agg({'banos':'median'})\nlngs = aux.T.to_dict('records').copy()\nlngs = lngs[0]\ndf.banos.fillna(df.tipodepropiedad.map(lngs), inplace = True)\ndf.banos.fillna(1, inplace=True)\n\naux = df.copy()\naux.metrostotales.fillna(0, inplace=True)\naux.metroscubiertos.fillna(0, inplace =True)\naux['metrostotalesCorregidos'] = aux[['metrostotales', 'metroscubiertos']].max(axis=1)\naux['metroscubiertosCorregidos'] = aux[['metrostotales', 'metroscubiertos']].min(axis=1)\ndf['metrostotales'] = aux['metrostotalesCorregidos']\ndf['metroscubiertos'] = aux['metroscubiertosCorregidos']\n\n\n\ndf['fecha'] = pd.to_datetime(df['fecha'])\ndf['dia'] = df.fecha.dt.day\ndf['mes'] = df.fecha.dt.month\ndf['anio'] = df.fecha.dt.year\ndf.drop(columns=['fecha'], inplace=True)\n\ndf['publicacionesPorCiudad'] = CountEncoder().fit_transform(df['ciudad'])\ndf['publicacionesPorProvincia'] = CountEncoder().fit_transform(df['provincia'])\n\ndf['publicacionesporZonaYAnio'] = df.groupby(['idzona','anio'])['antiguedad'].transform('count')\n\naux = df[['mes', 'anio']].copy()\naux['cantidad'] =  1\naux['cantidad'] = aux.groupby(['mes', 'anio']).transform('sum')\ndf['publicacionesEnMes'] = aux['cantidad']\n\naux = df[['metrostotales', 'tipodepropiedad']].copy()\naux['metrosPromedioPorTipo'] = aux.groupby('tipodepropiedad').transform('mean')\ndf['metrosPromedioPorTipo'] = aux['metrosPromedioPorTipo']\n\n\n\naux = df[['metrostotales', 'ciudad']].copy()\naux['metrosPromedioPorCiudad'] = aux.groupby('ciudad').transform('mean')\ndf['metrosPromedioPorCiudad'] = aux['metrosPromedioPorCiudad']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformador_categorias = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n])\n\ntransfomador_numero = Pipeline(steps=[('a', SimpleImputer(strategy = 'median'))])\n\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', transfomador_numero, ['antiguedad',\n       'habitaciones', 'garages', 'banos', 'metroscubiertos', 'metrostotales',\n       'idzona', 'lat', 'lng','adicionales',\n       'dia', 'mes', 'anio','publicacionesPorCiudad',\n       'publicacionesPorProvincia', 'publicacionesporZonaYAnio',\n       'publicacionesEnMes', 'metrosPromedioPorTipo',\n       'metrosPromedioPorCiudad']),\n        ('cat', transformador_categorias, ['tipodepropiedad', 'provincia', 'ciudad'])\n    ])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = als.FMRegression(n_iter=1000, init_stdev=0.1, rank=2, l2_reg_w=0.1, l2_reg_V=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"entrenamiento = df.head(TAMANIO_TRAIN).copy()\nprueba = df.tail(TAMANIO_TEST).copy()\nprueba.drop(columns=['precio'], inplace=True)\ntrain_x = entrenamiento.drop(columns=['precio']).copy()\ntrain_y = entrenamiento.precio.copy()\ntrain_x.tipodepropiedad = train_x.tipodepropiedad.astype(str)\ntrain_x.provincia = train_x.provincia.astype(str)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_pipeline.fit(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = my_pipeline.predict(train_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nping()\nmean_absolute_error(preds, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vemos que aún con los features utilizados en unas buenas estimaciones que tuvimos FM no da buen resultado.\n\nOPeero podríamos usarlo para clasificar o predecir texto\n\n### Trato de generar un feature que sea una aproximacion del precio en funcion del texto"},{"metadata":{"trusted":true},"cell_type":"code","source":"aux = dfCheto[[\"titulo\", \"descripcion\", \"direccion\", \"precio\"]].copy()\naux.fillna(\"\",inplace = True)\naux[\"texto\"] = aux[\"titulo\"].astype(str) + \" \" + aux[\"descripcion\"].astype(str) +\" \"+ aux[\"direccion\"].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\ntabla = {}\nfor c in string.punctuation + \"¿¡\":\n    tabla[ord(c)] = None\nfor c in string.ascii_uppercase:\n    tabla[ord(c)] = c.lower()\ndef sacar_puntuacion_y_minusculizar(s):\n    return s.translate(tabla)\n\naux[\"texto\"] = aux[\"texto\"].apply(sacar_puntuacion_y_minusculizar)\n\nfrom nltk.corpus import stopwords\npalabras_vacias = stopwords.words('spanish')\nimport unidecode\nfor i in range(len(palabras_vacias)):\n    palabras_vacias[i] = unidecode.unidecode(palabras_vacias[i])\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(strip_accents='unicode', stop_words=palabras_vacias)\n\ntransformador_texto = Pipeline(steps=[('tf-idf', vectorizer)])\nprocesador_texto = ColumnTransformer(transformers=[\n        ('text', transformador_texto, ['texto'])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = als.FMRegression(n_iter=250, init_stdev=0.1, rank=2, l2_reg_w=0.1, l2_reg_V=0.5)\npipeline_texto = Pipeline(steps=[('preprocessor', procesador_texto),\n                              ('model', model)\n                             ])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"entrenamiento = aux.head(TAMANIO_TRAIN).copy()\ntrain_x = entrenamiento[\"texto\"].copy()\ntrain_y = entrenamiento.precio.copy()\n\ntexto_procesado = transformador_texto.fit_transform(train_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texto_procesado.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(texto_procesado, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(texto_procesado)\nping()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nping()\nmean_absolute_error(preds, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prueba = aux.tail(TAMANIO_TEST).copy()\n#prueba.drop(columns=['precio'], inplace=True)\ntest_y = prueba[\"precio\"].copy()\ntexto_procesado_test = transformador_texto.transform(prueba[\"texto\"])\ntexto_procesado_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds2 = model.predict(texto_procesado_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nping()\nmean_absolute_error(preds2, test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ping()\npreds2 = np.absolute(preds2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.DataFrame(preds2, index=test.id, columns=['precio'])\nres.reset_index(inplace=True)\nres.columns = [\"id\", \"target\"]\ndisplay(res.head())\n# RMSLE=1.0249284784393988 ?\n\n\n# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"knn-results.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe\ncreate_download_link(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nentrenamiento = aux.head(TAMANIO_TRAIN).copy()\ntrain_x = entrenamiento[\"texto\"].copy()\ntrain_y = entrenamiento.precio.copy()\n\nX_train, X_test, y_train, y_test = train_test_split(\n   train_x , train_y, test_size=0.33, random_state=42)\n\nX_train_trans = transformador_texto.transform(X_train)\nX_test_trans = transformador_texto.transform(X_test)\n\nn_iter = 10\nstep_size = 2\nl2_reg_w = 0\nl2_reg_V = 0\n\nparams =100000\nfm = als.FMRegression(n_iter=0, init_stdev = 0.28, l2_reg=params, rank=2)\n# Allocates and initalizes the model parameter.\nfm.fit(X_train_trans, y_train)\n\nrmse_train = []\nrmse_test = []\nr2_score_train = []\nr2_score_test = []\nfor i in range(1, n_iter):\n    print(fm.get_params())\n    fm.fit(X_train_trans, y_train, n_more_iter=step_size)\n    params *= 1.10\n    fm.set_params(l2_reg=params, l2_reg_w=params, l2_reg_V=params)\n    \n    y_pred_test = fm.predict(X_test_trans)\n    y_pred_train = fm.predict(X_train_trans)\n\n    rmse_train.append(mean_absolute_error(y_pred_train, y_train))\n    rmse_test.append(mean_absolute_error(y_pred_test, y_test))\n\n    r2_score_train.append(r2_score(y_pred_train, y_train))\n    r2_score_test.append(r2_score(y_pred_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ping()\nx = np.arange(1, n_iter) * step_size\nfig, axes = plt.subplots(ncols=2, figsize=(15, 4))\naxes[0].plot(x, rmse_train, label='MAE-train', color='r', ls=\"--\")\naxes[0].plot(x, rmse_test, label='MAE-test', color='r')\n\naxes[1].plot(x, r2_score_train, label='R^2-train', color='b', ls=\"--\")\naxes[1].plot(x, r2_score_test, label='R^2-test', color='b')\n#axes[0].set_yscale('log')\n#axes[1].set_yscale('log')\n\naxes[0].set_ylabel('MAE', color='r')\naxes[1].set_ylabel('R^2', color='b')\naxes[0].legend()\naxes[1].legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_test[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"15426355.523708496 5 iter l2_reg=5, rank=8\n15199490.841102434 5 iter l2_reg=5, rank=2 #Empeoro\n16948064.1257035   5 iter l2_reg=5, rank=1 # Mas peor\n10969900.727196898 10 iter l2_reg=5, rank=8 #Mejor\n6385817.322031822  10 iter l2_reg=10, rank=8 #Mejor\n4593583.346254185  10 iter l2_reg=20, rank=8 #mejoro pero creo que debe overfittear\n2446010.0858626226 10 iter l2_reg=200, rank=8 #better\n2187496.2726370473 10 iter l2_reg=400, rank=8 #Mejoro pero falta\n1901927.550437147  10 iter l2_reg=1000, rank=8 #mejor\n1698296.3342013592 10 iter l2_reg=5000, rank=8 #mejor pero luego de 2 iteraciones comenzo a subir\n1584523.3392954106 10 iter l2_reg=5000, rank=2 #mejoro extrañamente\n1494395.7616398989 10 iter l2_reg=10000, rank=2\n1636825.7935355457 10 iter l2_reg=10000, rank=5 #peor\n1461863.6166319877 10 iter l2_reg=20000, rank=2 #mejor\n1309379.2221132882 10 iter l2_reg=100000, rank=2 #mejor\n1552464.7361177704 50 iter l2_reg=10000, rank=2 # empeoro\n1486699.8753103984 10 iter init_stdev = 0.3, l2_reg=10000, rank=2 #mejoro\n1519634.902997233  10 iter init_stdev = 0.5, l2_reg=10000, rank=2 #empeoro\n1552335.8695174404 10 iter init_stdev = 0.01, l2_reg=10000, rank=2 #empeoro\n1218251.7260080273 10 iter init_stdev = 0.3, l2_reg=200000, rank=2 #mejoro\n1127341.8178689817 10 iter init_stdev = 0.3, l2_reg=500000, rank=2 #mejoro pero me esta cansando\n1066328.1431204537 10 iter init_stdev = 0.3, l2_reg=1500000, rank=2 #Basta no lo logro bajar del millon\n1073065.6412416825 10 iter init_stdev = 0.25, l2_reg=1500000, rank=3 #empeoro\n1066728.843381326 10 iter init_stdev = 0.8, l2_reg=1500000, rank=3 #meh\n1042973.0560466803 20 iter init_stdev = 0.3, l2_reg=1300000, rank=2 #mejoro poniendole mas iteraciones\n1047046.1476110086 35 iter init_stdev = 0.3, l2_reg=1000000, rank=2 #todaia empeoro\n1056651.3892042716 35 iter init_stdev = 0.3, l2_reg=800000, rank=2 #todavia empeoro\n1049735.7639088458 60 iter init_stdev = 0.3, l2_reg=800000, rank=2 #sigue sin se mejor que el mejor que tuve\n1065502.5001307093 60 iter init_stdev = 0.3, l2_reg=800000, rank=2 #el rank lo unico que hace es empeorar todo\n1066526.916931029  60 iter init_stdev = 0.03, l2_reg=800000, rank=3 \n1041682.2088397375 10 iter step 5 size 5 init_stdev = 0.28, l2_reg=1000000, rank=2 update 0.75\n1041682.2088397375 10 iter step 5 size 5 init_stdev = 0.28, l2_reg=1000000, rank=2 update 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(fm.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}